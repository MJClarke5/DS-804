# -*- coding: utf-8 -*-
"""FinalProject_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LpufOpiD9j6zsFOjbGnK04Y9ZHpvNrug

## **Loading Libraries and Data**
"""

#Loading all libraries needed
install.packages("tm")
install.packages("wordcloud")
install.packages("tidytext")
install.packages("ggrepel")
library(readxl)
library(tidyverse)
library(tidyr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(ggrepel)

#Reading in data
posts = read_excel("/content/posts.xlsx")
comments = read_excel("/content/comments.xlsx")

print("POSTS: ")
glimpse(posts)
print("COMMENTS: ")
glimpse(comments)

"""# **Data Cleaning**

### Posts Missing Values
"""

#Find the number of missing values in all columns, only show columns with more than 0 missing values
columns_with_missing_values_posts = posts %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>% #count how many missing values across all columns
  pivot_longer(cols=everything(), names_to="Column", values_to="MissingValues") %>% #
  filter(MissingValues > 0)

#Find proportion of missing values for each column by dividing number of missing values by total
proportion_missing_posts = columns_with_missing_values_posts %>%
  mutate(ProportionMissing = MissingValues/nrow(posts) *100)

#Display the proportion of missing values
proportion_missing_posts

"""### Comments Missing Values"""

#Find the number of missing values in all columns, only show columns with more than 0 missing values
columns_with_missing_values_comments = comments %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>% #count how many missing values across all columns
  pivot_longer(cols=everything(), names_to="Column", values_to="MissingValues") %>% #
  filter(MissingValues > 0)

#Find proportion of missing values for each column by dividing number of missing values by total
proportion_missing_comments = columns_with_missing_values_comments %>%
  mutate(ProportionMissing = MissingValues/nrow(comments) *100)

#Display the proportion of missing values
proportion_missing_comments

"""The sentiment is missing from 10% of comments data

### Standardize Text Fields
"""

#Convert all text to lowercase for analysis
posts = posts %>%
  mutate(selftext = tolower(selftext)) %>%
  mutate(title = tolower(title))

comments = comments %>%
  mutate(body = tolower(body))

"""### Check for Duplicates (none)"""

# Check for duplicate rows
duplicates_posts = posts %>%
  distinct() %>%
  filter(duplicated(posts))

# Print the number of duplicate rows
print(nrow(duplicates_posts))

# Check for duplicate rows
duplicates_comments = comments %>%
  distinct() %>%
  filter(duplicated(comments))

# Print the number of duplicate rows
print(nrow(duplicates_comments))

"""### Outliers"""

#Check for outliers in posts score column
iqr_values_post_score = posts %>%
  summarise(
  Q1 = quantile(score, 0.25, na.rm=TRUE),
  Q3 = quantile(score, 0.75, na.rm=TRUE)
  ) %>%
    mutate(IQR=Q3-Q1,
      LowerBound = Q1-1.5*IQR,
      UpperBound = Q3+1.5*IQR) %>%
    select(LowerBound, UpperBound)

# Filter the data to find outliers
outliers_score = posts %>%
  filter(score < iqr_values_post_score$LowerBound | score > iqr_values_post_score$UpperBound) #filter to get values at both extremes

# Display the number of outliers
print("Post Score Outliers:")
outliers_score$score
print("LowerBound:")
print(iqr_values_post_score$LowerBound)
print("UpperBound:")
print(iqr_values_post_score$UpperBound)
print("Proportion of Outliers:")
print(nrow(outliers_score)/nrow(posts) *100)

#Check for outliers in comments score column
iqr_values_comments_score = comments %>%
  summarise(
  Q1 = quantile(score, 0.25, na.rm=TRUE),
  Q3 = quantile(score, 0.75, na.rm=TRUE)
  ) %>%
    mutate(IQR=Q3-Q1,
      LowerBound = Q1-1.5*IQR,
      UpperBound = Q3+1.5*IQR) %>%
    select(LowerBound, UpperBound)

# Filter the data to find outliers
outliers_score = comments %>%
  filter(score < iqr_values_comments_score$LowerBound | score > iqr_values_comments_score$UpperBound) #filter to get values at both extremes

# Display the number of outliers
print("Comments Score Outliers:")
outliers_score$score
print("LowerBound:")
print(iqr_values_comments_score$LowerBound)
print("UpperBound:")
print(iqr_values_comments_score$UpperBound)
print("Proportion of Outliers:")
print(nrow(outliers_score)/nrow(comments) *100)

#Check for outliers in comments sentiment column (none)
iqr_values_comments_sentiment = comments %>%
  summarise(
  Q1 = quantile(sentiment, 0.25, na.rm=TRUE),
  Q3 = quantile(sentiment, 0.75, na.rm=TRUE)
  ) %>%
    mutate(IQR=Q3-Q1,
      LowerBound = Q1-1.5*IQR,
      UpperBound = Q3+1.5*IQR) %>%
    select(LowerBound, UpperBound)

# Filter the data to find outliers
outliers_sentiment = comments %>%
  filter(sentiment < iqr_values_comments_sentiment$LowerBound | sentiment > iqr_values_comments_sentiment$UpperBound) #filter to get values at both extremes

# Display the number of outliers
print("Comments Score Outliers:")
outliers_sentiment$sentiment
print("LowerBound:")
print(iqr_values_comments_sentiment$LowerBound)
print("UpperBound:")
print(iqr_values_comments_sentiment$UpperBound)
print("Proportion of Outliers:")
print(nrow(outliers_sentiment)/nrow(comments) *100)

"""# Data Wrangling"""

#add length of comment
comments = comments %>%
  mutate(comment_length = sapply(strsplit(body, " "), length))

#Top 10 Subreddit with the most posts
subreddit_posts = posts %>%
  group_by(subreddit.name) %>%
  summarise(num_posts = n_distinct(id)) %>%
  arrange(desc(num_posts)) %>%
  slice_head(n=10)

subreddit_posts

#Top 10 Subreddit with the most comments
subreddit_comments = comments %>%
  group_by(subreddit.name) %>%
  summarise(num_comments = n_distinct(id)) %>%
  arrange(desc(num_comments)) %>%
  slice_head(n=10)

subreddit_comments

#Filter posts by wallstreetbest subreddit
wsb_posts = filter(posts, grepl("wallstreetbets", posts$subreddit.name))

#Filter comments by wallstreetbest subreddit
wsb_comments = filter(comments, grepl("wallstreetbets", comments$subreddit.name))

"""# Data Analysis"""

print(paste("Average Comment Sentiment Overall:", mean(comments$sentiment, na.rm=TRUE)))
print(paste("Average Comment Sentiment WSB:", mean(wsb_comments$sentiment, na.rm=TRUE)))

print(paste("Average Comment Score Overall:", mean(comments$score, na.rm=TRUE)))
print(paste("Average Comment Score WSB:", mean(wsb_comments$score, na.rm=TRUE)))

print(paste("Average Post Score Overall:", mean(posts$score, na.rm=TRUE)))
print(paste("Average Post Score WSB:", mean(wsb_posts$score, na.rm=TRUE)))

"""### Correlation Matrix"""

comments$subreddit_class = as.numeric(unclass(as.factor(comments$subreddit.name)))

# Select only the relevant columns
selected_data = comments %>%
  select(score, sentiment, subreddit.nsfw, comment_length, subreddit_class)

# Ensure all selected columns are numeric
selected_data = mutate_if(selected_data, is.character, is.numeric)

# excluding NA values.
correlation_matrix = cor(selected_data, use = "pairwise.complete.obs")

# View the correlation matrix
correlation_matrix

wsb_comments$subreddit_class = as.numeric(unclass(as.factor(wsb_comments$subreddit.name)))

# Select only the relevant columns
selected_data = wsb_comments %>%
    select(score, sentiment, comment_length, subreddit_class)

# Ensure all selected columns are numeric
selected_data = mutate_if(selected_data, is.character, is.numeric)

# excluding NA values.
correlation_matrix = cor(selected_data, use = "pairwise.complete.obs")

# View the correlation matrix
correlation_matrix

"""### Posts and Comments Over Time"""

posts_over_time = posts %>%
  group_by(Year=year(created_datetime), Month=month(created_datetime)) %>%
  summarise(num_posts = n_distinct(id)) %>%
  arrange(desc(num_posts))

#Display
posts_over_time

comments_over_time = comments %>%
  group_by(Year=year(created_datetime), Month=month(created_datetime)) %>%
  summarise(num_posts = n_distinct(id)) %>%
  arrange(desc(num_posts))

#Display
comments_over_time

"""# Text Analysis

### Word Clouds
"""

#Post title
# Create a text corpus
corpus = Corpus(VectorSource(posts$title))

# Define custom stop words including 'information'
custom_stop_words = c(stopwords("en"), "aapl", "stock", "apple", "stocks")

# Preprocess the text
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removeWords, custom_stop_words)

# Create a word cloud
set.seed(1234)
wordcloud(words=corpus, scale = c(3,.5), max.words=100, random.order=FALSE,
  colors=brewer.pal(8, "Dark2"))

#Post subtext
# Create a text corpus
corpus = Corpus(VectorSource(posts$selftext))

# Define custom stop words including 'information'
custom_stop_words = c(stopwords("en"), "aapl", "stock", "apple", "stocks", "removed", "deleted")

# Preprocess the text
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removeWords, custom_stop_words)

# Create a word cloud
set.seed(1234)
wordcloud(words=corpus, scale = c(3,.5), max.words=100, random.order=FALSE,
  colors=brewer.pal(8, "Dark2"))

#Comments
# Create a text corpus
corpus = Corpus(VectorSource(comments$body))

# Define custom stop words including 'information'
custom_stop_words = c(stopwords("en"), "aapl", "stock", "apple", "stocks")

# Preprocess the text
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removeWords, custom_stop_words)

# Create a word cloud
set.seed(1234)
wordcloud(words=corpus, scale = c(3,.5), max.words=100, random.order=FALSE,
  colors=brewer.pal(8, "Dark2"))

"""# Data Visualization"""

#Creating histogram plot for post score
ggplot(posts, aes(x = score)) +
  geom_histogram(bins = 100) +
  ylim(c(0, 150)) +
  xlim(c(0, 2000))

#Creating histogram plot for comments score
ggplot(comments, aes(x = score)) +
  geom_histogram(bins = 100) +
  ylim(c(0, 150)) +
  xlim(c(0, 2000))

#Creating histogram plot for comments score
ggplot(comments, aes(x = sentiment)) +
  geom_histogram(binwidth=.1)

#Create scatter plot of minimum and maximum salary
ggplot(comments, aes(x=score, y=sentiment)) +
  geom_point() +
  labs(title= "Scatterplot of Comment Score and Sentiment",
  x="Score",
  y="Sentiment") +
    theme_minimal()

#Do downvoted comments have a negative or positive sentiment?
comments_negative = comments %>%
  filter(score < 0)

ggplot(comments_negative, aes(x=score, y=sentiment)) +
  geom_point() +
  labs(title= "Scatterplot of Comment Score and Sentiment (Downvoted Comments)",
  x="Score",
  y="Sentiment") +
    theme_minimal()

"""### Boxplots"""

top_subreddits = comments[comments$subreddit.name %in% c("wallstreetbets","stocks","optionmillionaires","newsbotmarket", "forexhome"),]

ggplot(top_subreddits, aes(x=subreddit.name, y=sentiment)) +
  geom_boxplot() +
  labs(title= "Boxplots of Sentiment by Subreddit",
  x="Subreddit",
  y="Sentiment") +
    theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1))

"""### Line Graphs"""

#Group number of posts by date
posts_over_time = posts %>%
  group_by(DATE=date(created_datetime)) %>%
  summarise(num_post = n_distinct(id))

#Create line plot to display number of jobs over time
ggplot(posts_over_time, aes(x=DATE, y = num_post)) +
  geom_line(group = 1, color = "black") +
  labs(title= "Posts Over Time",
  x="Date",
  y="Number of Posts") +
    theme_minimal()

#Group number of comments by month
comments_over_time = comments %>%
  group_by(DATE=date(created_datetime)) %>%
  summarise(num_post = n_distinct(id))

#Create line plot to display number of jobs over time
ggplot(comments_over_time, aes(x=DATE, y = num_post)) +
  geom_line(group = 1, color = "black") +
  labs(title= "Comments Over Time",
  x="Date",
  y="Number of Comments") +
    theme_minimal()

#Group average sentiment by date
sentiment_over_time = comments %>%
  group_by(DATE=date(created_datetime)) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm=TRUE))

#Create line plot to display number of jobs over time
ggplot(sentiment_over_time, aes(x=DATE, y = avg_sentiment)) +
  geom_line(group = 1, color = "black") +
  labs(title= "Comment Sentiment Over Time",
  x="Date",
  y="Average Sentiment of Comments") +
    theme_minimal()

#Group average sentiment by date
wsb_sentiment_over_time = wsb_comments %>%
  group_by(DATE=date(created_datetime)) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm=TRUE))

#Create line plot to display number of jobs over time
ggplot(wsb_sentiment_over_time, aes(x=DATE, y = avg_sentiment)) +
  geom_line(group = 1, color = "black") +
  labs(title= "Comment Sentiment Over Time",
  x="Date",
  y="Average Sentiment of Comments") +
    theme_minimal()